<HTML>
<HEAD>
<TITLE>Sahil Garg's Web space</TITLE>
</HEAD>

<BODY>

<h3>
Sahil Garg, PhD
<br>
Machine Learning Researcher at Morgan Stanley
</h3>

<p>
<img src="./sahil3.jpeg" width="350" height="400" alt="Smiley face" align="left">


<h2>Research Interests</h2>
I enjoy conducting problem driven research. In the past 13 years, I had the privilege of collaborating with many researchers from diverse fields, and accordingly, some of the research interests are listed as below.

<h3>Deep Neural Nets</h3>
Generative AI, out-of-distribution detection, continual
learning, domain adaptation (distributional invariance), information-theoretic clustering or kNN search, transfer entropy or mutual information estimation, robust & interpretative classification, scaling laws for DNNs, highly noisy or irregular timeseries analysis, curriculum learning, multi-scale anomaly (fraud) detection in database systems.

<h3>Natural Language Processing</h3>
Relation extraction for cancer biology, dialog modeling for psychiatry, abstract meaning representations, discourse analysis for measuring thought disorder & complexity, metaphor detection, interpretative document summary with LLMs, curriculum pretraining of LLMs, OOD detection in LLMs.

<h3>Machine Learning</h3>
Nonstationary kernels, informative sensing, locality sensitive hashing for representation learning, convolution kernels for text, information theoretic representation learning, nearly unsupervised, attention mechanism in kernels via
nonstationarity, interpretative, data-efficiency, neuro-inspired, sparse modeling, network science.

<br>
<br>
<br>

<h2>Employment and Education</h2>
At present, I am enjoying <a href="https://www.morganstanley.com/about-us/technology/machine-learning-research-team">ML research at Morgan Stanley</a>, continuing my passion for problem-driven research. Previously, I explored the field of computational psychiatry as a postdoctoral fellow in the <a href="https://icahn.mssm.edu/">Icahn School of Medicine</a> at Mount Sinai under the mentorship of <a href="https://profiles.mountsinai.org/cheryl-corcoran">Prof. Cheryl Corcoran</a> and <a href="https://research.ibm.com/people/guillermo-cecchi">Dr. Guillermo Cecchi</a>. Prior to that, I was advised by <a href="https://scholar.google.com/citations?user=rJTwW0MAAAAJ&hl=en">Prof. Aram Galstyan</a> for my PhD thesis at USC. In the past life, I had the privilege of learning under the guidance of <a href="https://engineering.brown.edu/people/nora-ayanian">Prof. Nora Ayanian</a> at USC, <a href="https://www.iiitd.ac.in/amarjeet">Prof. Amarjeet Singh</a> at IIIT Delhi, and <a href="https://research.nvidia.com/person/fabio-ramos">Prof. Fabio Ramos</a>.

<h2> Selected Papers</h2>
<p>
Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI. Garg et al. 2024. <a href="https://arxiv.org/pdf/2404.07377.pdf">Preprint PDF</a>.
<p>
<p>
Empowering Time Series Analysis with Large Language Models: A Survey. Jiang et al. 2024. <a href="https://arxiv.org/abs/2402.03182">PDF</a>
<p>
<p>
Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting. Rasul et al. NeurIPS 2023 Workshop: R0-FoMo. <a href="https://arxiv.org/pdf/2310.08278.pdf">PDF</a>, <a href="https://github.com/time-series-foundation-models/lag-llama">Code</a>.
<p>
<p>
In- or Out-of-Distribution Detection via Dual Divergence Estimation. Garg et al. UAI-23. <a href="https://openreview.net/pdf?id=9Lo52ll_izs">PDF</a>, <a href="https://github.com/morganstanley/MSML/tree/main/papers/OOD_Detection_via_Dual_Divergence_Estimation">Code</a>.
<p>
Information Theoretic Clustering via Divergence Maximization among Clusters. Garg et al. UAI-23. <a href="https://openreview.net/pdf?id=HwxEI3u5Ui">PDF</a>, <a href="https://github.com/morganstanley/MSML/tree/main/papers/Clustering_via_Dual_Divergence_Maximization">Code</a>.
<p>
Estimating Transfer Entropy under Long Ranged Dependencies. Garg et al. UAI-22. <a href="https://openreview.net/pdf?id=SlWLvO8ice5">PDF</a>, <a href="https://github.com/morganstanley/MSML/tree/main/papers/Direct_Estimate_Transfer_Entropy">Code</a>.
<p>
Increased Metaphor Production in Open-Ended Speech Samples of Patients With Prodromal and Developed Schizophrenia Detected with NLP. Srivastava et al. Biological Psychiatry, 2022. <a href="https://www.biologicalpsychiatryjournal.com/article/S0006-3223(22)00232-3/fulltext"><PDF></a>
<p>
<p>
Negative symptoms and speech pauses in youths at clinical high risk for psychosis. Stanislawski et al. NPJ Schizophrenia, 2021. <a href="https://www.nature.com/articles/s41537-020-00132-1">PDF</a>
<p>
<p>
NERO: a biomedical named-entity (recognition) ontology with a large, annotated corpus reveals meaningful associations through text embedding. Wang et al. NPJ Systems Biology and Applications, 2021. <a href="https://www.nature.com/articles/s41540-021-00200-x">PDF</a>
<p>
<p>
Linking language features to clinical symptoms and multimodal imaging in individuals at clinical high risk for psychosis. Haas et al. European Psychiatry, 2020. <a href="https://www.cambridge.org/core/journals/european-psychiatry/article/linking-language-features-to-clinical-symptoms-and-multimodal-imaging-in-individuals-at-clinical-high-risk-for-psychosis/6E8A06E971162DAB55DDC7DCF54B6CC8">PDF</a>
<p>
Modeling Dialogues with Hashcode Representations: A Nonparametric Approach. Garg et al. AAAI-20. <a href="https://arxiv.org/pdf/1804.10188.pdf">PDF</a>.
<p>
Nearly-Unsupervised Hashcode Representations for Relation Extraction. Garg et al. EMNLP-19. <a href="https://arxiv.org/pdf/1909.03881.pdf">PDF</a>, <a href="https://github.com/sgarg87/nearly_unsupervised_hashcode_representations">Code.</a>
<p>
Kernelized Hashcode Representations for Relation Extraction. Garg et al. AAAI-19. <a href="https://arxiv.org/pdf/1711.04044.pdf">PDF</a>, <a href="">Code.</a>
<p>
Stochastic Learning of Nonstationary Kernels for Natural Language Modeling. Garg et al. 2017. <a href="https://arxiv.org/pdf/1801.03911.pdf">PDF</a>.
<p>
Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a Changing World. Sahil Garg*, Irina Rish, Guillermo Cecchi, Aurelie Lozano. IJCAI-17. <a href="https://arxiv.org/abs/1701.06106">PDF</a>, <a href="https://github.com/sgarg87/neurogenesis_inspired_dictionary_learning">Code.</a>
<p>
Extracting Biopathway Interactions using Semantic Parsing of Biomedical Text. Garg et al. AAAI-16. <a href="http://arxiv.org/abs/1512.01587">PDF</a>, <a href="https://github.com/sgarg87/big_mech_isi_gg">Code.</a>

<p>
Persistent Monitoring of Stochastic Spatio-temporal Phenomena with a Small Team of Robots. Garg et al. RSS-14. <a href="http://www.roboticsproceedings.org/rss10/p38.html">PDF</a>.

<p>
Learning Nonstationary Space-Time Models for Environmental Monitoring. Garg et al. AAAI-12. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4905">PDF</a>, <a href="https://github.com/sgarg87/nostill_gp">Code</a>.

<br>
<p>

<p>
See <a href="https://scholar.google.com/citations?user=Sz2mNx0AAAAJ&hl=en&oi=ao">Google Scholar</a> for an extended list of the papers.

<p>
This site is text-positive and defiantly retro (hand-crafted HTML 1.0).

<p>

<HR>

<ADDRESS>sahil.garg.cs at gmail.com</ADDRESS>
</BODY>
</HTML>